{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9aePYUtLu0kO9BuFdB8qY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudhab21/ML-LAB/blob/main/ML_Lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f80su-EjaOhE",
        "outputId": "5cb5a609-3d65-4a6b-e3cb-9b0853122bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "Environment is ready. Paths are set to your 'natural2' and 'engineered' folders.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "project_path = '/content/drive/MyDrive/GenomeDetector'\n",
        "natural_data_path = os.path.join(project_path, 'data', 'natural2')\n",
        "engineered_data_path = os.path.join(project_path, 'data', 'engineered')\n",
        "\n",
        "!pip install biopython\n",
        "\n",
        "print(\"Environment is ready. Paths are set to your 'natural2' and 'engineered' folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import product\n",
        "from Bio import SeqIO\n",
        "import glob\n",
        "import os\n",
        "import gzip\n",
        "\n",
        "def get_kmer_features(k):\n",
        "    \"\"\"Generates all possible k-mers for a given k.\"\"\"\n",
        "    letters = ['A', 'C', 'G', 'T']\n",
        "    return sorted([''.join(p) for p in product(letters, repeat=k)])\n",
        "\n",
        "def sequence_to_kmer_counts(sequence, k, kmer_features):\n",
        "    \"\"\"Converts a DNA sequence into a vector of k-mer counts.\"\"\"\n",
        "    kmer_counts = {kmer: 0 for kmer in kmer_features}\n",
        "    for i in range(len(sequence) - k + 1):\n",
        "        kmer = sequence[i:i+k].upper()\n",
        "        if kmer in kmer_counts:\n",
        "            kmer_counts[kmer] += 1\n",
        "    return list(kmer_counts.values())\n",
        "\n",
        "# --- Main Loading Logic ---\n",
        "print(\"--- Loading and Processing Your Datasets ---\")\n",
        "K_VALUE = 4\n",
        "kmer_features = get_kmer_features(K_VALUE)\n",
        "X, y = [], []\n",
        "\n",
        "# 1. Load Natural Data from 'natural2' (handles .fna and .fna.gz)\n",
        "print(\"Loading 'Natural' genomes from the 'natural2' folder...\")\n",
        "# The '*' wildcard finds files ending in .fna or .fna.gz\n",
        "search_pattern_natural = f\"{natural_data_path}/*.fna*\"\n",
        "natural_files = glob.glob(search_pattern_natural)\n",
        "\n",
        "for filepath in natural_files:\n",
        "    # Open the file correctly, whether it's gzipped or not\n",
        "    handle = gzip.open(filepath, \"rt\") if filepath.endswith(\".gz\") else open(filepath, \"r\")\n",
        "    for record in SeqIO.parse(handle, \"fasta\"):\n",
        "        if len(record.seq) > 1000:\n",
        "            X.append(sequence_to_kmer_counts(str(record.seq), K_VALUE, kmer_features))\n",
        "            y.append(0) # Label 0 for Natural\n",
        "    handle.close()\n",
        "\n",
        "# 2. Load Engineered Data from 'engineered'\n",
        "print(\"Loading 'Engineered' genomes from the 'engineered' folder...\")\n",
        "search_pattern_engineered = f\"{engineered_data_path}/*.fasta\"\n",
        "engineered_files = glob.glob(search_pattern_engineered)\n",
        "\n",
        "for filepath in engineered_files:\n",
        "    for record in SeqIO.parse(filepath, \"fasta\"):\n",
        "        if 200 < len(record.seq) < 50000:\n",
        "            X.append(sequence_to_kmer_counts(str(record.seq), K_VALUE, kmer_features))\n",
        "            y.append(1) # Label 1 for Engineered\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"\\n--- Data Processing Complete! ---\")\n",
        "print(f\"Shape of our final feature matrix X: {X.shape}\")\n",
        "print(f\"Shape of our final label vector y: {y.shape}\")\n",
        "print(f\"Number of Natural samples (class 0): {np.sum(y == 0)}\")\n",
        "print(f\"Number of Engineered samples (class 1): {np.sum(y == 1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDTT5bDzaeu8",
        "outputId": "f1504b15-3143-42ef-881c-c791dba41b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading and Processing Your Datasets ---\n",
            "Loading 'Natural' genomes from the 'natural2' folder...\n",
            "Loading 'Engineered' genomes from the 'engineered' folder...\n",
            "\n",
            "--- Data Processing Complete! ---\n",
            "Shape of our final feature matrix X: (17168, 256)\n",
            "Shape of our final label vector y: (17168,)\n",
            "Number of Natural samples (class 0): 16302\n",
            "Number of Engineered samples (class 1): 866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Model Training and Comparison (A3) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import all the classifiers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# ---- Modularized Functions ----\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Trains a model and returns its performance on train and test sets.\"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_metrics = {\n",
        "        'Train Accuracy': accuracy_score(y_train, y_pred_train),\n",
        "        'Train F1-Score': f1_score(y_train, y_pred_train)\n",
        "    }\n",
        "    test_metrics = {\n",
        "        'Test Accuracy': accuracy_score(y_test, y_pred_test),\n",
        "        'Test F1-Score': f1_score(y_test, y_pred_test)\n",
        "    }\n",
        "    return train_metrics, test_metrics\n",
        "\n",
        "# --- Main Program Logic ---\n",
        "if 'X' not in locals() or X.shape[0] == 0:\n",
        "    print(\"ERROR: Data variables 'X' and 'y' not found. Please run the data loading cell first.\")\n",
        "else:\n",
        "    print(\"--- A3: Comparing Various Classifiers ---\")\n",
        "\n",
        "    # Some models (like SVM and MLP) work better with scaled data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # The \"Model Zoo\"\n",
        "    classifiers = {\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "        \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        \"CatBoost\": CatBoostClassifier(random_state=42, verbose=0),\n",
        "        \"Naive Bayes\": GaussianNB(),\n",
        "        \"SVM\": SVC(random_state=42),\n",
        "        \"MLP (Neural Net)\": MLPClassifier(random_state=42, max_iter=500)\n",
        "    }\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for name, model in classifiers.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        train_metrics, test_metrics = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # Combine results\n",
        "        result_row = {'Model': name}\n",
        "        result_row.update(train_metrics)\n",
        "        result_row.update(test_metrics)\n",
        "        results_list.append(result_row)\n",
        "\n",
        "    # Create and display a professional table using pandas\n",
        "    results_df = pd.DataFrame(results_list)\n",
        "    print(\"\\n--- Tabulated Results ---\")\n",
        "    print(results_df.to_string())\n",
        "\n",
        "    print(\"\\n--- Observation ---\")\n",
        "    print(\"Ensemble models like Random Forest and XGBoost generally perform the best and show good generalization.\")\n",
        "    print(\"A large gap between Train F1 and Test F1 indicates potential overfitting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZK5DZP6ajBp",
        "outputId": "063cb60b-edf4-4e97-fdb7-319f73236d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- A3: Comparing Various Classifiers ---\n",
            "Training Decision Tree...\n",
            "Training Random Forest...\n",
            "Training AdaBoost...\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [16:57:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CatBoost...\n",
            "Training Naive Bayes...\n",
            "Training SVM...\n",
            "Training MLP (Neural Net)...\n",
            "\n",
            "--- Tabulated Results ---\n",
            "              Model  Train Accuracy  Train F1-Score  Test Accuracy  Test F1-Score\n",
            "0     Decision Tree        1.000000        1.000000       0.992040       0.919132\n",
            "1     Random Forest        1.000000        1.000000       0.994176       0.939271\n",
            "2          AdaBoost        0.983440        0.820235       0.980392       0.775056\n",
            "3           XGBoost        1.000000        1.000000       0.997864       0.978389\n",
            "4          CatBoost        1.000000        1.000000       0.996894       0.968504\n",
            "5       Naive Bayes        0.333611        0.129376       0.341487       0.128916\n",
            "6               SVM        0.949571        0.000000       0.949524       0.000000\n",
            "7  MLP (Neural Net)        0.994008        0.939698       0.992623       0.928571\n",
            "\n",
            "--- Observation ---\n",
            "Ensemble models like Random Forest and XGBoost generally perform the best and show good generalization.\n",
            "A large gap between Train F1 and Test F1 indicates potential overfitting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owb77W-9alGj",
        "outputId": "4c3a1452-3bc7-481f-fea5-1aabbe0b0ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Hyperparameter Tuning (A2) ---\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "print(\"\\n--- A2: Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
        "\n",
        "# We'll tune the RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the \"dials\" to turn (hyperparameters) and the range of values to try\n",
        "# This is a distribution, not a fixed grid\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),\n",
        "    'max_depth': [None] + list(randint(10, 50).rvs(5)),\n",
        "    'min_samples_leaf': randint(1, 5)\n",
        "}\n",
        "\n",
        "# n_iter=10 means it will try 10 random combinations\n",
        "# cv=3 means it will use 3-fold cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Run the search\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n--- RandomizedSearchCV Results ---\")\n",
        "print(f\"Best Parameters Found: {random_search.best_params_}\")\n",
        "print(f\"Best Cross-Validated F1-Score: {random_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIbpL-fBaoei",
        "outputId": "bcf13b20-e046-4e76-b0be-bb93c6f0fb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- A2: Hyperparameter Tuning with RandomizedSearchCV ---\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "\n",
            "--- RandomizedSearchCV Results ---\n",
            "Best Parameters Found: {'max_depth': np.int64(37), 'min_samples_leaf': 1, 'n_estimators': 51}\n",
            "Best Cross-Validated F1-Score: 0.9923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Explainable AI with SHAP (O1) ---\n",
        "!pip install shap\n",
        "\n",
        "import shap\n",
        "\n",
        "print(\"\\n--- O1: Using SHAP for Model Explainability ---\")\n",
        "\n",
        "# We'll use the powerful XGBoost model for our explanation\n",
        "# (Tree-based models are fastest with SHAP)\n",
        "model_to_explain = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss').fit(X_train, y_train)\n",
        "\n",
        "# Create the explainer\n",
        "explainer = shap.TreeExplainer(model_to_explain)\n",
        "# Calculate SHAP values for the test set (can take a minute)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Get the k-mer feature names for our plots\n",
        "feature_names = get_kmer_features(K_VALUE)\n",
        "\n",
        "# --- Plot 1: Global Feature Importance ---\n",
        "print(\"\\nSHAP Summary Plot (Global Feature Importance):\")\n",
        "print(\"This plot shows the most important k-mers overall. Each dot is a sample.\")\n",
        "print(\"High feature values are red, low are blue.\")\n",
        "shap.summary_plot(shap_values, X_test, feature_names=feature_names)\n",
        "\n",
        "# --- Plot 2: Explanation of a Single Prediction ---\n",
        "# Let's explain why the model made a specific prediction for the first \"Engineered\" sample in the test set\n",
        "engineered_idx = np.where(y_test == 1)[0][0]\n",
        "\n",
        "print(f\"\\nSHAP Force Plot for a single 'Engineered' prediction (Sample {engineered_idx}):\")\n",
        "print(\"This plot shows which features PUSHED the prediction higher (towards Engineered) and which PULLED it lower.\")\n",
        "shap.initjs() # Initialize javascript for plotting\n",
        "shap.force_plot(\n",
        "    explainer.expected_value,\n",
        "    shap_values[engineered_idx, :],\n",
        "    X_test[engineered_idx, :],\n",
        "    feature_names=feature_names\n",
        ")"
      ],
      "metadata": {
        "id": "o4GP1gTCaq7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Explainable AI with LIME (O2) ---\n",
        "!pip install lime\n",
        "\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "print(\"\\n--- O2: Using LIME for Local Model Explainability ---\")\n",
        "\n",
        "# LIME needs a function that takes data and returns prediction probabilities\n",
        "# Our CatBoost model is a good choice here\n",
        "model_to_explain_lime = CatBoostClassifier(random_state=42, verbose=0).fit(X_train, y_train)\n",
        "predict_fn = lambda x: model_to_explain_lime.predict_proba(x)\n",
        "\n",
        "# Create the LIME explainer\n",
        "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train,\n",
        "    feature_names=feature_names,\n",
        "    class_names=['Natural', 'Engineered'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "# --- Explanation 1: Why did the model think this was ENGINEERED? ---\n",
        "engineered_idx = np.where(y_test == 1)[0][0]\n",
        "print(f\"\\nLIME Explanation for one 'Engineered' sample (Sample {engineered_idx}):\")\n",
        "exp_engineered = explainer_lime.explain_instance(\n",
        "    data_row=X_test[engineered_idx],\n",
        "    predict_fn=predict_fn,\n",
        "    num_features=6 # Show the top 6 features\n",
        ")\n",
        "exp_engineered.show_in_notebook(show_table=True)\n",
        "\n",
        "\n",
        "# --- Explanation 2: Why did the model think this was NATURAL? ---\n",
        "natural_idx = np.where(y_test == 0)[0][0]\n",
        "print(f\"\\nLIME Explanation for one 'Natural' sample (Sample {natural_idx}):\")\n",
        "exp_natural = explainer_lime.explain_instance(\n",
        "    data_row=X_test[natural_idx],\n",
        "    predict_fn=predict_fn,\n",
        "    num_features=6\n",
        ")\n",
        "exp_natural.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "kbdBN8lKasp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}